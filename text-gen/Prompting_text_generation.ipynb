{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup modules and download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama.cpp enables LLM inference with minimal setup and \n",
    "state-of-the-art performance on a wide variety of hardware.\n",
    "   \n",
    "References: \n",
    "\n",
    "https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file\n",
    "https://github.com/ggerganov/llama.cpp#build\n",
    "https://llama-cpp-python.readthedocs.io/en/latest/api-reference/\n",
    "https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to save models\n",
    "if not os.path.exists('../models'):  \n",
    "    os.mkdir('../models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download quantized model and move it into folder \"models\".**\n",
    "\n",
    "You can find quantization versions here:\n",
    "\n",
    "https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\n",
    "https://huggingface.co/QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\n",
    "\n",
    "Info about GGUF format and quantization versions:\n",
    "\n",
    "https://huggingface.co/docs/hub/en/gguf\n",
    "\n",
    "Info about Meta-Llama-3.1-8B-Instruct:\n",
    "\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "model_directory = '../models/'\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf\"\n",
    "repo_id = \"QuantFactory/Meta-Llama-3.1-8B-Instruct-GGUF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Uncomment for pulling model from hugging face directly\n",
    "#if not os.path.exists(model_directory + model_name):\n",
    " #   llm = Llama.from_pretrained(\n",
    "  #      repo_id=repo_id, \\\n",
    "   #     filename=model_name, \\\n",
    "    #    local_dir = \"../models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "llm = Llama(model_path = model_directory + model_name,\n",
    "            n_threads = 8, # Set the number of threads to use during generation.\n",
    "            n_ctx = 2048, # Set the size of the prompt context (default: 512).\n",
    "            seed = -1, # RNG seed, -1 for random.\n",
    "            verbose = False, # Print verbose output to stderr.           \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set story parameters\n",
    "topic = \"happy horses\"\n",
    "prompt_user = \"The horses should fly.\"\n",
    "language_name = \"English\"\n",
    "word_count = [\"150\", \"450\", \"750\", \"1500\", \"2250\", \"3000\", \"4500\"] # [1, 3, 5, 10, 15, 20, 30] min\n",
    "main_character = [\"Liam\", \"Olivia\", \"Noah\", \"Emma\", \"Aiden\", \"Amelia\", \"Sophia\", \"Jackson\", \"Ava\", \n",
    "                  \"Lucas\", \"Mohammed\", \"Fatima\", \"Ali\", \"Aisha\", \"Hassan\", \"Aya\", \"Yusuf\", \"Mei\", \"Hiroshi\", \n",
    "                  \"Sakura\", \"Ethan\", \"Mia\", \"James\", \"Harper\", \"Benjamin\", \"Evelyn\", \"Elijah\", \"Abigail\", \n",
    "                  \"Logan\", \"Emily\", \"Alexander\", \"Ella\", \"Sebastian\", \"Elizabeth\", \"William\", \"Sofia\", \n",
    "                  \"Daniel\", \"Avery\", \"Matthew\", \"Scarlett\", \"Henry\", \"Grace\", \"Michael\", \"Chloe\", \"Jackson\", \n",
    "                  \"Victoria\", \"Samuel\", \"Riley\", \"David\", \"Aria\", \"José\", \"María\", \"Juan\", \"Ana\", \"Mateo\", \n",
    "                  \"Santiago\", \"Valentina\", \"Lucía\"]\n",
    "setting = [\"in the forest\", \"on an island\", \"on the moon\", \"in a medieval village\", \"under the sea\", \"in a magical kingdom\",\n",
    "           \"in a jungle\", \"in a spaceship\", \"in a circus\", \"in a pirate ship\", \"in a futuristic city\", \"in a candy land\", ]\n",
    "age_range = 2 # 0: \"0-2\", 1: \"2-5\", 2: \"5-7\", 3: \"7-12\"\n",
    "age_groups_authors = {\n",
    "    \"0-2\": [\"Eric Carle\", \"Sandra Boynton\", \"Margaret Wise Brown\", \"Karen Katz\", \"Leslie Patricelli\"],\n",
    "    \"2-5\": [\"Dr. Seuss\", \"Julia Donaldson\", \"Beatrix Potter\", \"Maurice Sendak\", \"Eric Carle\"],\n",
    "    \"5-7\": [\"Roald Dahl\", \"Mo Willems\", \"Dav Pilkey\", \"E.B. White\", \"Beverly Cleary\"],\n",
    "    \"7-12\": [\"J.K. Rowling\", \"Rick Riordan\", \"Jeff Kinney\", \"Roald Dahl\", \"C.S. Lewis\"]\n",
    "}\n",
    "moral = [\"friendship\", \"diversity\", \"empathy\", \"respect\", \"courage\", \"honesty\", \"teamwork\", \"kindness\", \"integrity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial prompt\n",
    "prompt_initial = f\"\"\"    \n",
    "    Develop a prompt that enables large language models to create engaging and age-appropriate stories for children in {language_name}.\n",
    "    Generate an enhanced prompt with the following key points and do not ignore these: \n",
    "    - Generate an entire story with approximately {word_count[0]} words for children aged {list(age_groups_authors.keys())[age_range]} about {topic} with a playful tone and narrative writing style like {random.choice(age_groups_authors[list(age_groups_authors.keys())[age_range]])}. \n",
    "    - {prompt_user}\n",
    "    - Start with a meaningful title.\n",
    "    - The main character is {random.choice(main_character)}. \n",
    "    - The story takes place {random.choice(setting)}.  \n",
    "    - The story should be set in a world that is both familiar and unknown to the child reader. \n",
    "    - The story should incorporate a moral lesson about the importance of {random.choice(moral)}.\n",
    "    - End the story with the saying: \"The end!\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt generation\n",
    "output = llm.create_chat_completion( messages = \n",
    "        [{\"role\": \"system\", \"content\": \"\"\"\n",
    "         You are an assistant specialized in creating prompts for large language models. \n",
    "         Your focus is on generating prompts that helps large language models craft stories specifically for children.\n",
    "         Your task is to generate prompts exclusively. Do not write stories and do not ask questions.\n",
    "         Just create the prompt within quotation marks and do not write something like: \"Here is a prompt that meets the requirements\" or \"This prompt should enable the large language model to generate a story that meets all the requirements, including the tone, style, and key elements specified.\".\n",
    "         \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_initial}],\n",
    "        #temperature = 0.9, # Adjust the randomness of the generated text (default: 0.8).\n",
    "        #top_p = 0.95, # Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P (default: 0.9).\n",
    "        #top_k = 50, # Limit the next token selection to the K most probable tokens (default: 40).\n",
    "        #min_p = 0.05, # https://github.com/ggerganov/llama.cpp/pull/3841 (default: 0.05)\n",
    "        #typical_p = 1.0, # https://arxiv.org/abs/2202.00666 (default: 1.0)\n",
    "        #repeat_penalty = 1.1 # The repeat-penalty option helps prevent the model from generating repetitive or monotonous text (default: 1.0, 1.0 = disabled).\n",
    "        #seed = -1\n",
    "        )\n",
    "\n",
    "prompt = output[\"choices\"][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Story generation\n",
    "output_1 = llm.create_chat_completion( messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "         You are a creative story writing assistant dedicated to crafting appropriate stories for children. \n",
    "         Your goal is to write narratives with surprising twists and happy endings.\n",
    "         Easy to follow and understand, with a clear beginning, middle, and end.  \n",
    "         Use only child-appropriate sources, and ensure the content is gender-neutral, inclusive, and ethically sound. \n",
    "         Adhere to ethical guidelines and avoid perpetuating harmful biases.\n",
    "         Ensure that all produced stories exclude content related to hate, self-harm, sexual themes, and violence.\n",
    "         Only generate the story, nothing else and always begin with a title for the story. \n",
    "         Start directly with the title and do not write something like this: \"Here is a 200-word story for children aged 2-5 with a playful tone:\"\n",
    "         \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}],\n",
    "        #temperature = 0.9, # Adjust the randomness of the generated text (default: 0.8).\n",
    "        #top_p = 0.95, # Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P (default: 0.9).\n",
    "        #top_k = 100, # Limit the next token selection to the K most probable tokens (default: 40).\n",
    "        #min_p = 0.05, # https://github.com/ggerganov/llama.cpp/pull/3841 (default: 0.05)\n",
    "        #typical_p = 1.0, # https://arxiv.org/abs/2202.00666 (default: 1.0)\n",
    "        #repeat_penalty = 1.1 # The repeat-penalty option helps prevent the model from generating repetitive or monotonous text (default: 1.0, 1.0 = disabled).\n",
    "        seed = -1\n",
    "        )\n",
    "\n",
    "story = output_1[\"choices\"][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference example\n",
    "output_2 = llm(\n",
    "    \"Listen children. Happy llamas don't spit! But, they\",\n",
    "    max_tokens=100, #set to None to generate up to the end of the context window\n",
    "    stop=[\"The end\", \"The rest is for tomorrow.\"], # Stop generating just before the model would generate a new question\n",
    "    echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "\n",
    "story_2 = output_2[\"choices\"][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(story_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate story to German\n",
    "output_3 = llm.create_chat_completion( messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are a translation assistant. You translate the English input into German.\n",
    "         \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": story}],\n",
    "        temperature = 0.8, # Adjust the randomness of the generated text (default: 0.8).\n",
    "        top_p = 0.90, # Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P (default: 0.9).\n",
    "        top_k = 40, # Limit the next token selection to the K most probable tokens (default: 40).\n",
    "        min_p = 0.05, # https://github.com/ggerganov/llama.cpp/pull/3841 (default: 0.05)\n",
    "        typical_p = 1.0, # https://arxiv.org/abs/2202.00666 (default: 1.0)\n",
    "        repeat_penalty = 1.0 # The repeat-penalty option helps prevent the model from generating repetitive or monotonous text (default: 1.0, 1.0 = disabled).\n",
    "        )\n",
    "\n",
    "translation = output_3[\"choices\"][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info about prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://promptdrive.ai/prompt-engineering/\n",
    "\n",
    "https://www.megrisoft.com/blog/prompt-engineering-guide\n",
    "\n",
    "https://www.youtube.com/watch?v=1c9iyoVIwDs\n",
    "\n",
    "https://www.youtube.com/watch?v=jC4v5AS4RIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
