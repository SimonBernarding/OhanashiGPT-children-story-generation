{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIY Colab on text-to-image with flux1.schnell from Black Forest Lab\n",
    "In August 2024, Black Forest Lab introduced their new pretrained flux1 model. Meanwhile it's used by X(Twitter)'s Grok.\n",
    "\n",
    "The model itself has 12B parameters and requires a GPU to get it run in a reasonable time. It's recommended to have at least 12 GB memory on your CPU and 12 GB memory on your GPU.\n",
    "In the following you can click through two different versions to get flux running:\n",
    "\n",
    "1) A already quantized version of the transformer.\n",
    "2) The original model components from Black Forest Labs uploaded to HuggingFace, which are then quantized. The pipeline is built ony by one. One will also learn how to save quantized models in torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements and import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Run already quantized version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to install the necessary requirements and import the libraries we are going to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip --upgrade\n",
    "!pip install numpy==1.26.4\n",
    "!pip install accelerate\n",
    "!pip install git+https://github.com/huggingface/diffusers\n",
    "!pip install optimum-quanto\n",
    "!pip install transformers --upgrade \n",
    "\n",
    "import torch # necessary to check the device\n",
    "# identify which device is used (cuda = GPU, cpu = CPU only, mps = Mac)\n",
    "device: str = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "if device == 'cpu':\n",
    "    !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "elif device == 'cuda':\n",
    "    !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "elif device == 'mps':\n",
    "    !pip3 install torch torchvision torchaudio\n",
    "else:\n",
    "    print(\"device unknown\")\n",
    "# exception: cu124 necessary for google colab no matter if T4 GPU enabled or CPU only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from optimum.quanto import freeze, qfloat8, quantize\n",
    "\n",
    "from diffusers import FluxTransformer2DModel \n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "from transformers import T5EncoderModel\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook in google colab, execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and quantize different pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loading and saving path for models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = './models/text-to-image/flux.1-schnell' # saving path\n",
    "model = \"black-forest-labs/FLUX.1-schnell\" # official model flux1.-schnell from Blackforest (not quantized)\n",
    "model_tr = \"https://huggingface.co/Kijai/flux-fp8/blob/main/flux1-schnell-fp8.safetensors\" # quantized transformer from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create necessary folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{cache_dir}'): \n",
    "  os.makedirs(f'{cache_dir}')\n",
    "# saving folder for images\n",
    "if not os.path.exists('./figs'): \n",
    "  os.makedirs('./figs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and requantize transformer.\n",
    "Note: You may run out of CPU memory here, since the file is first completely loaded into the CPU memory and is about 12 GB big (CPU ram > 12 GB necessary). If you ran out of memory, try method 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = FluxTransformer2DModel.from_single_file(model_tr, \n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        cache_dir = cache_dir,\n",
    "                                                        #local_files_only=True # once you have downloaded the model, you can force the use of these downloaded models instead of downloading them each time you run the program.\n",
    ")\n",
    "quantize(transformer, weights=qfloat8)\n",
    "freeze(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and requantize text_encoder_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2 = T5EncoderModel.from_pretrained(model,\n",
    "                                                subfolder=\"text_encoder_2\",\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                cache_dir=cache_dir,\n",
    "                                                #local_files_only=True\n",
    ")\n",
    "quantize(text_encoder_2, weights=qfloat8)\n",
    "freeze(text_encoder_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up pipe line with main model and the two quantized models (transformer & text_encoder_2). When running on cuda (GPU) there are some more \"tricks\" to lower the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(model,\n",
    "                                    transformer=None,\n",
    "                                    text_encoder_2=None,\n",
    "                                    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "pipe.transformer = transformer\n",
    "pipe.text_encoder_2 = text_encoder_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cuda (GPU) use ONLY to save some VRAM on GPU to get the code running with VRAM < 16 GB. Depending on your GPU you should try to either use \"enable_model_cpu_offload\" or all the three other lines of code all together. Try out which option runs faster (or at all since it's a very GPU consuming model). Just the first line tends to be faster but you need more GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    # pipe.enable_model_cpu_offload() # offloads modules to CPU on a submodule level (rather than model level)\n",
    "    pipe.enable_sequential_cpu_offload() # when using non-quantized versions to make it run with VRAM 4-32 GB\n",
    "    pipe.vae.enable_slicing() # when using non-quantized versions to make it run with VRAM 4-32 GB\n",
    "    pipe.vae.enable_tiling() # when using non-quantized versions to make it run with VRAM 4-32 GB\n",
    "else: \n",
    "    pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and create image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for the image. Most important: prompt which should describe the picture as closely as possible. You can also describe something in the foreground, in the background, etc. and define the style, e.g. photorealistic, high definition, water color style, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Dog in Space on a flying carpet. Behind there are cats. In the background there is a snow covered mountain and the moon.\"\n",
    "height, width = 1024, 1024 # standard = 1024x1024\n",
    "num_inference_steps = 4  # number of iterations, 4 gives decent results and should be considered as minimum; people on HuggingFace, GitHub and Reddit: ~15-50 iterations. Check for yourself to get a good tradeoff between speed and quality\n",
    "generator = torch.Generator(device).manual_seed(12345) # set seed for repeatable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    guidance_scale=0.0, # must be 0.0 for flux1.-schnell, may be 3.5 for flux1.-dev but up to 7.0 --> higher guidance scale forces the model to keep closer to the prompt at the expense of image quality and may introduce artefacts\n",
    "    height=height,\n",
    "    width=width,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    max_sequence_length=256, #256 is max for flux1.-schnell; maximum sequence length to use with the prompt\n",
    "    generator=generator\n",
    ").images[0]\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save(f\"figs/Kijai_qt-qte2_{num_inference_steps}_{height}_{width}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Run original models and define pipeline components one by one, then quantize them manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on the work from https://gist.github.com/AmericanPresidentJimmyCarter/873985638e1f3541ba8b00137e7dacd9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the first version, we are going to install all necessary requirements and import the corresponding libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip --upgrade\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 # if cuda 12.4 does not work, go to https://pytorch.org/get-started/locally/ and select the version that fits your OS.\n",
    "!pip install transformers --upgrade\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf\n",
    "!pip install accelerate\n",
    "!pip install git+https://github.com/huggingface/diffusers\n",
    "!pip install optimum-quanto\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from optimum.quanto import freeze, qfloat8, quantize #, qint4\n",
    "\n",
    "from diffusers import FlowMatchEulerDiscreteScheduler, AutoencoderKL\n",
    "from diffusers import FluxTransformer2DModel\n",
    "from diffusers import FluxPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5TokenizerFast, AutoModelForCausalLM\n",
    "#from safetensors.torch import save_file, load_file\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook in google colab, execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and quantize different pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loading and saving path for models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfl_repo = \"black-forest-labs/FLUX.1-schnell\" # official model flux1.-schnell from Blackforest (not quantized)\n",
    "revision = \"refs/pr/7\" #refs/pr/1 works\n",
    "model_tr = \"https://huggingface.co/Kijai/flux-fp8/blob/main/flux1-schnell-fp8.safetensors\" # quantized transformer from HuggingFace\n",
    "cache_dir = './models/text-to-image/flux.1-schnell' # saving path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create necessary folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'{cache_dir}'): \n",
    "  os.makedirs(f'{cache_dir}')\n",
    "# saving folder for images\n",
    "if not os.path.exists('./figs'): \n",
    "  os.makedirs('./figs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to quantize the original transformer introduced by Black Forest Labs. It's 24 GB big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original, not quantized transformer from flux schnell = 24 GB\n",
    "\n",
    "print(\"start loading transformer...\")\n",
    "transformer = FluxTransformer2DModel.from_pretrained(bfl_repo,\n",
    "                                                     subfolder=\"transformer\",\n",
    "                                                     torch_dtype=torch.bfloat16,\n",
    "                                                     revision=revision,\n",
    "                                                     cache_dir = cache_dir,\n",
    "                                                     #local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"start quantizing transformer...\")\n",
    "# quantizing qfloat8 works, you may also want to try qint4 and see if it works\n",
    "#quantize(transformer, weights=qint4, exclude=[\"proj_out\", \"x_embedder\", \"norm_out\", \"context_embedder\"])\n",
    "quantize(transformer, weights=qfloat8)\n",
    "\n",
    "# print(\"start freezing transformer...\")\n",
    "freeze(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, we can still use the already quantized transformer (12 GB file size) and requantize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp8 quantized transformer = 12 GB\n",
    "\n",
    "print(\"start loading transformer...\")\n",
    "transformer = FluxTransformer2DModel.from_single_file(model_tr,\n",
    "                                                      torch_dtype=torch.bfloat16,\n",
    "                                                      cache_dir = cache_dir,\n",
    "                                                      #local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"start quantizing transformer...\")\n",
    "quantize(transformer, weights=qfloat8)\n",
    "\n",
    "print(\"start freezing transformer...\")\n",
    "freeze(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the quantized transformer (if you went for the already quantized transformer, you can skip the next two cells)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(transformer)\n",
    "torch.save(transformer, f'{cache_dir}' + '/' + 'transformer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and load it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading(transformer)\n",
    "transformer = torch.load(f'{cache_dir}' + '/' + 'transformer.pt')\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will quantize the text_encoder_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start loading text_encoder_2...\")\n",
    "text_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo,\n",
    "                                                subfolder=\"text_encoder_2\", \n",
    "                                                torch_dtype=torch.bfloat16, \n",
    "                                                revision=revision\n",
    ")\n",
    "\n",
    "print(\"start quantizing text_encoder_2...\")\n",
    "quantize(text_encoder_2, weights=qfloat8)\n",
    "\n",
    "print(\"start freezing text_encoder_2...\")\n",
    "freeze(text_encoder_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the quantized text_encoder_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving (text_encoder_2)\n",
    "torch.save(text_encoder_2, f'{cache_dir}' + '/' + 'text_encoder_2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading quantized text_encoder_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading (text_encoder_2)\n",
    "text_encoder_2 = torch.load(f'{cache_dir}' + '/' + 'text_encoder_2.pt')\n",
    "text_encoder_2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load remaining pipeline components, one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will be loading all the other pipeline components one by one instead of loading it from one single file. This means, you also get to see all the components of the flux pipeline. Cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder=\"scheduler\", revision=revision)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.bfloat16)\n",
    "tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder=\"tokenizer_2\", torch_dtype=torch.bfloat16, revision=revision)\n",
    "vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder=\"vae\", torch_dtype=torch.bfloat16, revision=revision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have all model components loaded, we can now set up the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline(\n",
    "    scheduler=scheduler,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder_2=None,\n",
    "    tokenizer_2=tokenizer_2,\n",
    "    vae=vae,\n",
    "    transformer=None,\n",
    ")\n",
    "\n",
    "pipe.text_encoder_2 = text_encoder_2\n",
    "pipe.transformer = transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can apply some tricks to use less VRAM when using a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    # pipe.enable_model_cpu_offload() # offloads modules to CPU on a submodule level (rather than model level)\n",
    "    pipe.enable_sequential_cpu_offload() # when using non-quantized versions to make it run with VRAM 4-32 GB\n",
    "    pipe.vae.enable_slicing() # when using non-quantized versions to make it run with VRAM 4-32 GB\n",
    "    pipe.vae.enable_tiling() # when using non-quantized versions to make it run with VRAM 4-32 GB\n",
    "else: \n",
    "    pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and create image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter defintion for the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Dog in Space on a flying carpet. Behind there are cats. In the background there is a snow covered mountain and the moon.\"\n",
    "height, width = 1024, 1024 # standard = 1024x1024\n",
    "num_inference_steps = 4  # number of iterations, 4 gives decent results and should be considered as minimum; people on HuggingFace, GitHub and Reddit: ~15-50 iterations. Check for yourself to get a good tradeoff between speed and quality\n",
    "generator = torch.Generator(device).manual_seed(12345) # set seed for repeatable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    guidance_scale=0.0, # must be 0.0 for flux1.-schnell, may be 3.5 for flux1.-dev but up to 7.0 --> higher guidance scale forces the model to keep closer to the prompt at the expense of image quality\n",
    "    height=height,\n",
    "    width=width,\n",
    "    #output_type=\"pil\",\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    max_sequence_length=128, #256 is max for flux1.-schnell; maximum sequence length to use with the prompt\n",
    "    generator=generator\n",
    ").images[0]\n",
    "\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save(f\"figs/OneByOne_qt-qte2_{num_inference_steps}_{height}_{width}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
